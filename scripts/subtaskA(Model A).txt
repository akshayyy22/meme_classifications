!pip install transformers datasets torchvision tqdm
!pip install git+https://github.com/mlfoundations/open_clip.git

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
from PIL import Image
import os

train_csv = "/content/drive/MyDrive/meme_classifications/ocr_text/train/STask_A_train.csv"
train_img_dir = "/content/drive/MyDrive/meme_classifications/data/train/subtaskA"

# Load the CSV first
train_df = pd.read_csv(train_csv)
train_df.dropna(subset=["text", "label"], inplace=True)

# Filter rows with missing images
def filter_missing_images(df, img_root):
    valid_rows = []
    for _, row in df.iterrows():
        label = row["label"]
        folder = "Hate" if label == 1 else "No Hate"
        img_path = os.path.join(img_root, folder, f"{row['index']}")
        if os.path.exists(img_path):
            valid_rows.append(row)
        else:
            print(f"Missing image: {img_path}")
    return pd.DataFrame(valid_rows)

train_df = filter_missing_images(train_df, train_img_dir)


# Optional: Save filtered CSV for debugging
# train_df.to_csv("filtered_train.csv", index=False)

# Image loader
def load_image(img_name):
    path = os.path.join(train_img_dir, img_name)
    return Image.open(path).convert("RGB")


from transformers import RobertaTokenizer, RobertaForSequenceClassification

# This will download automatically and cache the model
tokenizer = RobertaTokenizer.from_pretrained("roberta-base")
model = RobertaForSequenceClassification.from_pretrained("roberta-base", num_labels=2)



import torch
from torch.utils.data import Dataset
import open_clip
from transformers import RobertaTokenizer

class MemeDataset(Dataset):
    def __init__(self, df, img_root, tokenizer, transform):
        self.df = df
        self.img_root = img_root
        self.tokenizer = tokenizer
        self.transform = transform

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        row = self.df.iloc[idx]

        # Handle the label for training and evaluation differently
        if "label" in row:
            label = row["label"]  # This will be present in training, not in evaluation
        else:
            label = -1  # Or any placeholder for eval data where labels are not available

        # Dynamically determine the folder based on the label
        folder = "Hate" if label == 1 else "No Hate" if label != -1 else None

        # Construct the full image path
        img_path = os.path.join(self.img_root, folder, f"{row['index']}") if folder else os.path.join(self.img_root, f"{row['index']}")

        # Open the image
        image = Image.open(img_path).convert("RGB")
        image = self.transform(image)

        # Tokenize the text
        text = row["text"]
        tokens = self.tokenizer(text, padding="max_length", truncation=True, return_tensors="pt")

        if label != -1:
            return {
                "pixel_values": image,
                "input_ids": tokens["input_ids"].squeeze(0),
                "attention_mask": tokens["attention_mask"].squeeze(0),
                "label": torch.tensor(label, dtype=torch.long)
            }
        else:
            return {
                "pixel_values": image,
                "input_ids": tokens["input_ids"].squeeze(0),
                "attention_mask": tokens["attention_mask"].squeeze(0)
            }



import torch.nn as nn
from transformers import RobertaModel

class FusionClassifier(nn.Module):
    def __init__(self, clip_embed_dim=512, text_hidden_size=768, num_classes=2):
        super().__init__()
        self.text_model = RobertaModel.from_pretrained("roberta-base")
        self.classifier = nn.Sequential(
            nn.Linear(clip_embed_dim + text_hidden_size, 512),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(512, num_classes)
        )

    def forward(self, image_features, input_ids, attention_mask):
        text_output = self.text_model(input_ids=input_ids, attention_mask=attention_mask)
        text_feat = text_output.pooler_output

        fused = torch.cat((image_features, text_feat), dim=1)
        return self.classifier(fused)



from torch.utils.data import DataLoader
from sklearn.metrics import f1_score
from tqdm import tqdm

# Load CLIP
clip_model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='openai')
clip_model.eval().cuda()

tokenizer = RobertaTokenizer.from_pretrained("roberta-base")
train_ds = MemeDataset(train_df, train_img_dir, tokenizer, preprocess)

# Split
from sklearn.model_selection import train_test_split
train_indices, val_indices = train_test_split(range(len(train_ds)), test_size=0.1, stratify=train_df["label"], random_state=42)
train_subset = torch.utils.data.Subset(train_ds, train_indices)
val_subset = torch.utils.data.Subset(train_ds, val_indices)

train_loader = DataLoader(train_subset, batch_size=16, shuffle=True)
val_loader = DataLoader(val_subset, batch_size=16)

# Model
model = FusionClassifier().cuda()
optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)
criterion = nn.CrossEntropyLoss()

# Training loop
for epoch in range(4):
    model.train()
    for batch in tqdm(train_loader):
        with torch.no_grad():
            img_feats = clip_model.encode_image(batch["pixel_values"].cuda()).float()

        out = model(img_feats, batch["input_ids"].cuda(), batch["attention_mask"].cuda())
        loss = criterion(out, batch["label"].cuda())

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    # Evaluation
    model.eval()
    preds, true = [], []
    with torch.no_grad():
        for batch in val_loader:
            img_feats = clip_model.encode_image(batch["pixel_values"].cuda()).float()
            out = model(img_feats, batch["input_ids"].cuda(), batch["attention_mask"].cuda())
            pred = torch.argmax(out, dim=1).cpu().tolist()
            preds.extend(pred)
            true.extend(batch["label"].cpu().tolist())

    f1 = f1_score(true, preds)
    print(f"Epoch {epoch+1} F1 Score: {f1:.4f}")


torch.save(model.state_dict(), "/content/drive/MyDrive/CASE2025/models/fusion_model.pth")


import pandas as pd
import torch
from torch.utils.data import DataLoader
from tqdm import tqdm
import json
from transformers import RobertaTokenizer
import open_clip  # assuming you are using CLIP

# Load model
model = FusionClassifier().cuda()  # Your model definition
model.load_state_dict(torch.load("/content/drive/MyDrive/CASE2025/models/fusion_model.pth"))
model.eval()
print("Model loaded!")

# Load eval data (No 'label' column, just text and index)
eval_csv = "/content/drive/MyDrive/meme_classifications/ocr_text/eval/SubtaskA_eval.csv"
eval_img_dir = "/content/drive/MyDrive/meme_classifications/data/eval/subtaskA"
eval_df = pd.read_csv(eval_csv).dropna(subset=["text"]).sort_values("index")

# Prepare dataset and loader (no 'label' for evaluation)
eval_ds = MemeDataset(eval_df, eval_img_dir, tokenizer, preprocess)  # MemeDataset defined earlier
eval_loader = DataLoader(eval_ds, batch_size=16)

# Make predictions
all_preds = []
with torch.no_grad():
    for batch in tqdm(eval_loader):
        img_feats = clip_model.encode_image(batch["pixel_values"].cuda()).float()
        out = model(img_feats, batch["input_ids"].cuda(), batch["attention_mask"].cuda())
        pred = torch.argmax(out, dim=1).cpu().tolist()
        all_preds.extend(pred)

# Prepare the submission file
eval_df["prediction"] = all_preds
submission = eval_df[["index", "prediction"]].to_dict(orient="records")
submission = sorted(submission, key=lambda x: x["index"])



# Save JSONL
submission_path = "/content/drive/MyDrive/CASE2025/meme-classification/submission/submission.json"
with open(submission_path, "w") as f:
    for row in submission:
        json.dump(row, f)
        f.write("\n")

        

