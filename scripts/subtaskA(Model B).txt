#!/usr/bin/env python

# Install dependencies
!pip install transformers datasets torchvision tqdm xgboost lightgbm scikit-learn joblib --quiet
!pip install git+https://github.com/mlfoundations/open_clip.git --quiet

from google.colab import drive
drive.mount('/content/drive')

# Imports
import os
import json
import joblib
import torch
import torch.nn as nn
import pandas as pd
import numpy as np
from PIL import Image
from tqdm import tqdm
from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.preprocessing import StandardScaler
from sklearn.neural_network import MLPClassifier
from sklearn.ensemble import GradientBoostingClassifier
import xgboost as xgb
import lightgbm as lgb
from torch.utils.data import Dataset, DataLoader
from transformers import RobertaTokenizer, RobertaModel, RobertaForSequenceClassification
import open_clip

# Load Data
train_csv = "/content/drive/MyDrive/meme_classifications/ocr_text/train/STask_A_train.csv"
train_img_dir = "/content/drive/MyDrive/meme_classifications/data/train/subtaskA"
eval_csv = "/content/drive/MyDrive/meme_classifications/ocr_text/eval/SubtaskA_eval.csv"
eval_img_dir = "/content/drive/MyDrive/meme_classifications/data/eval/subtaskA"

train_df = pd.read_csv(train_csv).dropna(subset=["text", "label"])
eval_df = pd.read_csv(eval_csv).dropna(subset=["text"]).sort_values("index")

# Filter missing images
def filter_missing_images(df, img_root):
    valid_rows = []
    for _, row in df.iterrows():
        label = row["label"]
        folder = "Hate" if label == 1 else "No Hate"
        img_path = os.path.join(img_root, folder, f"{row['index']}")
        if os.path.exists(img_path):
            valid_rows.append(row)
    return pd.DataFrame(valid_rows)

train_df = filter_missing_images(train_df, train_img_dir)

# Dataset
class MemeDataset(Dataset):
    def __init__(self, df, img_root, tokenizer, transform):
        self.df = df
        self.img_root = img_root
        self.tokenizer = tokenizer
        self.transform = transform

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        label = row.get("label", -1)
        folder = "Hate" if label == 1 else "No Hate" if label != -1 else None
        img_path = os.path.join(self.img_root, folder, f"{row['index']}") if folder else os.path.join(self.img_root, f"{row['index']}")

        image = Image.open(img_path).convert("RGB")
        image = self.transform(image)

        tokens = self.tokenizer(row["text"], padding="max_length", truncation=True, return_tensors="pt")

        out = {
            "pixel_values": image,
            "input_ids": tokens["input_ids"].squeeze(0),
            "attention_mask": tokens["attention_mask"].squeeze(0)
        }
        if label != -1:
            out["label"] = torch.tensor(label, dtype=torch.long)
        return out

# Fusion Model
class FusionClassifier(nn.Module):
    def __init__(self, clip_embed_dim=512, text_hidden_size=768, num_classes=2):
        super().__init__()
        self.text_model = RobertaModel.from_pretrained("roberta-base")
        self.classifier = nn.Sequential(
            nn.Linear(clip_embed_dim + text_hidden_size, 512),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(512, num_classes)
        )

    def forward(self, image_features, input_ids, attention_mask):
        text_output = self.text_model(input_ids=input_ids, attention_mask=attention_mask)
        text_feat = text_output.pooler_output
        fused = torch.cat((image_features, text_feat), dim=1)
        return self.classifier(fused)

# Load CLIP and Tokenizer
clip_model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='openai')
clip_model.eval().cuda()
tokenizer = RobertaTokenizer.from_pretrained("roberta-base")

# Train-test split
train_ds = MemeDataset(train_df, train_img_dir, tokenizer, preprocess)
train_idx, val_idx = train_test_split(range(len(train_ds)), test_size=0.1, stratify=train_df["label"], random_state=42)
train_subset = torch.utils.data.Subset(train_ds, train_idx)
val_subset = torch.utils.data.Subset(train_ds, val_idx)
train_loader = DataLoader(train_subset, batch_size=16, shuffle=True)
val_loader = DataLoader(val_subset, batch_size=16)

# Train Fusion Model
model = FusionClassifier().cuda()
optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)
criterion = nn.CrossEntropyLoss()

for epoch in range(4):
    model.train()
    for batch in tqdm(train_loader):
        with torch.no_grad():
            img_feats = clip_model.encode_image(batch["pixel_values"].cuda()).float()
        out = model(img_feats, batch["input_ids"].cuda(), batch["attention_mask"].cuda())
        loss = criterion(out, batch["label"].cuda())
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    model.eval()
    preds, true = [], []
    with torch.no_grad():
        for batch in val_loader:
            img_feats = clip_model.encode_image(batch["pixel_values"].cuda()).float()
            out = model(img_feats, batch["input_ids"].cuda(), batch["attention_mask"].cuda())
            pred = torch.argmax(out, dim=1).cpu().tolist()
            preds.extend(pred)
            true.extend(batch["label"].cpu().tolist())
    print(f"Epoch {epoch+1} F1: {f1_score(true, preds):.4f}")

# Save Fusion Model
torch.save(model.state_dict(), "/content/drive/MyDrive/CASE2025/models/fusion_model.pth")

# === Classical ML Models ===
vectorizer = CountVectorizer(ngram_range=(1, 2), max_features=5000)
x_syntactic = vectorizer.fit_transform(train_df['text']).toarray()
y = train_df['label'].values

scaler = StandardScaler()
x_scaled = scaler.fit_transform(x_syntactic)

ml_models = {
    "mlp": MLPClassifier(hidden_layer_sizes=(128,), max_iter=300),
    "xgb": xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss'),
    "lgbm": lgb.LGBMClassifier(),
    "gbm": GradientBoostingClassifier()
}

for name, clf in ml_models.items():
    clf.fit(x_scaled, y)
    joblib.dump(clf, f"/content/drive/MyDrive/CASE2025/models/{name}.pkl")

joblib.dump(vectorizer, "/content/drive/MyDrive/CASE2025/models/vectorizer.pkl")
joblib.dump(scaler, "/content/drive/MyDrive/CASE2025/models/scaler.pkl")

# === Evaluation & Submission ===
eval_ds = MemeDataset(eval_df, eval_img_dir, tokenizer, preprocess)
eval_loader = DataLoader(eval_ds, batch_size=16)

# Load trained models
model.load_state_dict(torch.load("/content/drive/MyDrive/CASE2025/models/fusion_model.pth"))
model.eval()
vectorizer = joblib.load("/content/drive/MyDrive/CASE2025/models/vectorizer.pkl")
scaler = joblib.load("/content/drive/MyDrive/CASE2025/models/scaler.pkl")

ml_models = {
    name: joblib.load(f"/content/drive/MyDrive/CASE2025/models/{name}.pkl")
    for name in ["mlp", "xgb", "lgbm", "gbm"]
}

fusion_preds = []
with torch.no_grad():
    for batch in tqdm(eval_loader):
        img_feats = clip_model.encode_image(batch["pixel_values"].cuda()).float()
        out = model(img_feats, batch["input_ids"].cuda(), batch["attention_mask"].cuda())
        fusion_preds.extend(torch.softmax(out, dim=1).cpu().tolist())

text_features = scaler.transform(vectorizer.transform(eval_df['text']).toarray())
ensemble_preds = []

for i in range(len(eval_df)):
    preds = [ml_models[m].predict_proba([text_features[i]])[0] for m in ml_models]
    ml_avg = np.mean(preds, axis=0)
    fusion_prob = fusion_preds[i]
    final = 0.5 * ml_avg + 0.5 * fusion_prob
    ensemble_preds.append(np.argmax(final))

eval_df['prediction'] = ensemble_preds
submission = eval_df[['index', 'prediction']].sort_values("index").to_dict(orient="records")

with open("/content/drive/MyDrive/CASE2025/meme-classification/submission/submission.json", "w") as f:
    for row in submission:
        json.dump(row, f)
        f.write("\n")
